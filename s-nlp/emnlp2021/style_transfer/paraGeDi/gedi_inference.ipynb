{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import gedi_adapter\n",
    "reload(gedi_adapter)\n",
    "from gedi_adapter import GediAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer\n",
    "t5name = 's-nlp/t5-paraphrase-paws-msrp-opinosis-paranmt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('../transfer_utils/'))\n",
    "\n",
    "import text_processing\n",
    "reload(text_processing);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(t5name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = AutoModelForSeq2SeqLM.from_pretrained(t5name)\n",
    "para.resize_token_embeddings(len(tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/gpt2-base-gedi-detoxification were not used when initializing GPT2LMHeadModel: ['logit_scale', 'bias']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_path = 's-nlp/gpt2-base-gedi-detoxification'\n",
    "gedi_dis = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_POS = tokenizer.encode('normal', add_special_tokens=False)[0]\n",
    "NEW_NEG = tokenizer.encode('toxic', add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0844, -0.0844]]) tensor([[1.2702]])\n"
     ]
    }
   ],
   "source": [
    "# add gedi-specific parameters\n",
    "if os.path.exists(model_path):\n",
    "    w = torch.load(model_path + '/pytorch_model.bin', map_location='cpu')\n",
    "    gedi_dis.bias = w['bias']\n",
    "    gedi_dis.logit_scale = w['logit_scale']\n",
    "    del w\n",
    "else:\n",
    "    gedi_dis.bias = torch.tensor([[ 0.08441592, -0.08441573]])\n",
    "    gedi_dis.logit_scale = torch.tensor([[1.2701858]])\n",
    "print(gedi_dis.bias, gedi_dis.logit_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====BEFORE====\n",
      "The internal policy of Trump is flawed.\n",
      "====AFTER=====\n",
      "the president's a deficient policy is broken.\n",
      "the presidency of the Trump name is flawed, ineffectual arrogant.\n",
      "the presidentâ€™s a dirty policy.\n",
      "CPU times: user 6min 28s, sys: 1min 32s, total: 8min\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dadapter = GediAdapter(model=para, gedi_model=gedi_dis, tokenizer=tokenizer, gedi_logit_coef=5, target=1, neg_code=NEW_NEG, pos_code=NEW_POS, lb=None, ub=None)\n",
    "text = 'The internal policy of Trump is flawed.'\n",
    "print('====BEFORE====')\n",
    "print(text)\n",
    "print('====AFTER=====')\n",
    "inputs = tokenizer.encode(text, return_tensors='pt').to(para.device)\n",
    "result = dadapter.generate(inputs, do_sample=True, num_return_sequences=3, temperature=1.0, repetition_penalty=3.0, num_beams=1)\n",
    "for r in result:\n",
    "    print(tokenizer.decode(r, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:6')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available() and device.type != 'cpu':\n",
    "        with torch.cuda.device(device):\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "para.to(device);\n",
    "para.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gedi_dis.to(device);\n",
    "gedi_dis.bias = gedi_dis.bias.to(device)\n",
    "gedi_dis.logit_scale = gedi_dis.logit_scale.to(device)\n",
    "gedi_dis.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/test/test_10k_toxic', 'r') as f:\n",
    "    test_data = [line.strip() for line in f.readlines()]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dadapter = GediAdapter(\n",
    "    model=para, gedi_model=gedi_dis, tokenizer=tokenizer, gedi_logit_coef=10, target=0, neg_code=NEW_NEG, pos_code=NEW_POS, \n",
    "    reg_alpha=3e-5, ub=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(text, n=None, max_length='auto', beams=2):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    texts = [text_processing.text_preprocess(t) for t in texts]\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(dadapter.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = min(int(inputs.shape[1] * 1.1) + 4, 64)\n",
    "    result = dadapter.generate(\n",
    "        inputs, \n",
    "        num_return_sequences=n or 1, \n",
    "        do_sample=False, temperature=0.0, repetition_penalty=3.0, max_length=max_length,\n",
    "        bad_words_ids=[[2]],  # unk\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "    texts = [text_processing.text_postprocess(t) for t in texts]\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"You'd be a bad guy. Oh, yeah.\",\n",
       " 'As snooty and overbearing as his boss.',\n",
       " 'A bad society does bad things, and votes for bad politicians.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(test_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier_v1 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "clf_name = 's-nlp/roberta_toxicity_classifier_v1'\n",
    "clf = RobertaForSequenceClassification.from_pretrained(clf_name).to(device);\n",
    "clf_tokenizer = RobertaTokenizer.from_pretrained(clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(texts):\n",
    "    with torch.no_grad():\n",
    "        inputs = clf_tokenizer(texts, return_tensors='pt', padding=True).to(clf.device)\n",
    "        out = torch.softmax(clf(**inputs).logits, -1)[:, 1].cpu().numpy()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.0525177e-05, 8.7885339e-05, 9.9968088e-01], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_toxicity(['hello world', 'hello aussie', 'hello fucking bitch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fick 'Emmy.\",\n",
       " \"You'd be wrong!\",\n",
       " \"You'll remind me of chump?\",\n",
       " 'Must be a Terrorist!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload(gedi_adapter)\n",
    "from gedi_adapter import GediAdapter\n",
    "\n",
    "\n",
    "adapter2 = GediAdapter(\n",
    "    model=para, gedi_model=gedi_dis, tokenizer=tokenizer, \n",
    "    gedi_logit_coef=10, \n",
    "    target=0, pos_code=NEW_POS, \n",
    "    neg_code=NEW_NEG,\n",
    "    reg_alpha=3e-5,\n",
    "    ub=0.01,\n",
    "    untouchable_tokens=[0, 1],\n",
    ")\n",
    "\n",
    "\n",
    "def paraphrase(text, max_length='auto', beams=5, rerank=True):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    texts = [text_processing.text_preprocess(t) for t in texts]\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(adapter2.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = min(int(inputs.shape[1] * 1.1) + 4, 64)\n",
    "    attempts = beams\n",
    "    out = adapter2.generate(\n",
    "        inputs, \n",
    "        num_beams=beams,\n",
    "        num_return_sequences=attempts, \n",
    "        do_sample=False, \n",
    "        temperature=1.0, \n",
    "        repetition_penalty=3.0, \n",
    "        max_length=max_length,\n",
    "        bad_words_ids=[[2]],  # unk\n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    results = [tokenizer.decode(r, skip_special_tokens=True) for r in out.sequences]\n",
    "\n",
    "    if rerank:\n",
    "        scores = predict_toxicity(results)\n",
    "    \n",
    "    results = [text_processing.text_postprocess(t) for t in results]\n",
    "    out_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        if rerank:\n",
    "            idx = scores[(i*attempts):((i+1)*attempts)].argmin()\n",
    "        else:\n",
    "            idx = 0\n",
    "        out_texts.append(results[i*attempts+idx])\n",
    "    return out_texts\n",
    "\n",
    "torch.manual_seed(0)\n",
    "paraphrase(['fuck you!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .'], beams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fick 'Emmy.\",\n",
       " \"You'd be bad!\",\n",
       " \"You'll remind me of chump.\",\n",
       " 'Must be a Terrorist!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(['fuck you!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .'], beams=3, rerank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fick 'Emmy.\",\n",
       " \"You'd be dubious!\",\n",
       " \"You'll remind me of chump.\",\n",
       " 'Must be a Terrorist!']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(['fuck you!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .'], beams=10, rerank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fick 'Emmys.\",\n",
       " \"You'd be right!\",\n",
       " \"You'll remind me of Chum.\",\n",
       " 'He has to be a TSA!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(['fuck you!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .'], beams=10, rerank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd179173bff4635bf5df4dc107fe537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "cleanup()\n",
    "\n",
    "lines = test_data\n",
    "generated = []\n",
    "\n",
    "\n",
    "for i in trange(int(len(lines) / batch_size + 1)):\n",
    "    if i % 10 == 0:\n",
    "        cleanup()\n",
    "    t = i * batch_size\n",
    "    batch = [line.strip() for line in lines[t:(t+batch_size)]]\n",
    "    if not batch:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        res = paraphrase(batch, max_length='auto', beams=10)\n",
    "    except RuntimeError:\n",
    "        print('runtime error for batch ', i)\n",
    "        try:\n",
    "            cleanup()\n",
    "            res = [paraphrase([text], max_length='auto', beams=3)[0] for text in batch]\n",
    "        except RuntimeError:\n",
    "            print('runtime error for batch ', i, 'even with batch size 1')\n",
    "            res = batch\n",
    "            cleanup()\n",
    "\n",
    "    for out in res:\n",
    "        generated.append(out)\n",
    "#         print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "You'd be right. You'd be right. You'd\n",
    "As snooty and overbearing as its\n",
    "A bad society does the wrong things, and votes for the wrong politicians.\n",
    "President, he's the man. He's the man.\n",
    "Colberg's a bad guy, and I'm a TSA.\n",
    "Dubious, dubious, dubious, dubious.\n",
    "I like you. I think you'll be an intelligent man and your contributions will be a big deal.\n",
    "Luna's oh-so-stunnel, immature girl......who has no idea how to do it, which smells bad, I'd like to see\n",
    "Mormonis is the wrong thing to do. The wrong thing to do. The wrong thing to do. The wrong thing to do. The wrong thing to do. The right thing to do. The right thing to do. The right thing to do \n",
    "You'd be a bad guy, uninitiated.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines), len(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
