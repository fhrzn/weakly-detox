{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnparallelDataset(Dataset):\n",
    "    def __init__(self, path, data_dir='../data/classification/skoltech-jigsaw/'):\n",
    "        super().__init__()\n",
    "        \n",
    "        data_path = os.path.join(data_dir, path)\n",
    "        \n",
    "        self.data_list = []\n",
    "        self.eos = \" <|endoftext|>\"\n",
    "        \n",
    "        df = pd.read_csv(data_path, sep='\\t', names=['labels', 'text'])\n",
    "        df['labels'] = df.labels.apply(lambda x: 'toxic' if x == 1 else 'normal')\n",
    "        \n",
    "        for row in tqdm(df.iterrows(), desc=f'Reading {path}'):\n",
    "            self.data_list.append(f'{row[1][\"labels\"]}: {row[1][\"text\"]}{self.eos}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.data_list[item]\n",
    "    \n",
    "\n",
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, path, data_dir='../data/paraphrase/'):\n",
    "        super().__init__()\n",
    "        \n",
    "        data_path = os.path.join(data_dir, path)\n",
    "        \n",
    "        self.data_list = []\n",
    "        self.eos = \" <|endoftext|>\"\n",
    "        \n",
    "        df = pd.read_csv(data_path, sep='\\t')\n",
    "        \n",
    "        for row in tqdm(df.iterrows(), desc=f'Reading {path}'):\n",
    "            self.data_list.append(f'paraphrase: {row[1][\"source\"]} >>> {row[1][\"backtranslate\"]}{self.eos}')\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.data_list[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(path, mode='unparallel'):\n",
    "    if mode == 'unparallel':\n",
    "        dataset = UnparallelDataset(path)\n",
    "    elif mode == 'paraphrase':\n",
    "        dataset = ParaphraseDataset(path)\n",
    "    else:\n",
    "        raise NotImplementedError('available mode: [unparallel, paraphrase]')\n",
    "        \n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, loader, batch_size, tokenizer, model, device):\n",
    "    \n",
    "    batch_counter = 0\n",
    "    sumloss = 0\n",
    "\n",
    "    num_steps = epochs * len(loader)\n",
    "    pb = tqdm(range(num_steps))\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(f'Epoch {e+1}')\n",
    "        \n",
    "        for step, txt in enumerate(loader):\n",
    "            txt = torch.tensor(tokenizer.encode(txt[0]))\n",
    "            txt = txt.unsqueeze(0).to(device)\n",
    "            outputs = model(txt, labels=txt)\n",
    "            loss, _ = outputs[:2]\n",
    "            loss.backward()\n",
    "            sumloss += loss.item()\n",
    "            \n",
    "            if step % batch_size == 0:\n",
    "                batch_counter += 1\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "                \n",
    "            if batch_counter == 10:\n",
    "                print(f'Total Loss: {sumloss}')\n",
    "                batch_counter = 0\n",
    "                sumloss = 0\n",
    "                \n",
    "            pb.update(1)\n",
    "                \n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, name):\n",
    "    print('saving model...')\n",
    "    torch.save(model.state_dict(), f'{name}.pt')\n",
    "    \n",
    "    \n",
    "def load_model():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e06015ae8e4bf2b0ff90f31c476432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading train.txt: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 300\n",
    "MAX_SEQ_LEN = 128\n",
    "MODEL_PATH = '../model/unparallel.pt'\n",
    "DATA_FILE = 'train.txt'  # 'paraphrase_ref.csv'\n",
    "\n",
    "\n",
    "TOKENIZER, MODEL = load_model()\n",
    "LOADER = get_data_loader(DATA_FILE)\n",
    "\n",
    "DEVICE = 'cuda:4' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MODEL.to(DEVICE)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7689001e05e14d8ab2c3fb79797c6217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Total Loss: 6556.524174451828\n",
      "Total Loss: 7024.49076461792\n",
      "Total Loss: 6517.577111721039\n",
      "Total Loss: 5871.979538440704\n",
      "Total Loss: 5385.574621915817\n",
      "Total Loss: 5006.7227239608765\n",
      "Total Loss: 4820.105655789375\n",
      "Total Loss: 4736.162583589554\n",
      "Total Loss: 4616.006004571915\n",
      "Total Loss: 4508.450771689415\n",
      "Total Loss: 4485.800826787949\n",
      "Total Loss: 4511.129070997238\n",
      "Total Loss: 4407.7837607860565\n",
      "Total Loss: 4490.502493619919\n",
      "Total Loss: 4411.379061102867\n",
      "Total Loss: 4390.180621385574\n",
      "Total Loss: 4375.2236849069595\n",
      "Total Loss: 4408.710793435574\n",
      "Total Loss: 4413.848687887192\n",
      "Total Loss: 4375.677514910698\n",
      "Total Loss: 4349.256544589996\n",
      "Total Loss: 4385.362689256668\n",
      "Total Loss: 4315.6337769031525\n",
      "Total Loss: 4336.66351890564\n",
      "Total Loss: 4273.727068543434\n",
      "Total Loss: 4336.140874028206\n",
      "Total Loss: 4304.023709774017\n",
      "Total Loss: 4283.8170664310455\n",
      "Total Loss: 4289.819076538086\n",
      "Total Loss: 4274.579474329948\n",
      "Total Loss: 4317.073293685913\n",
      "Total Loss: 4331.883008003235\n",
      "Total Loss: 4311.820141077042\n",
      "Total Loss: 4300.88151884079\n",
      "Total Loss: 4263.714606881142\n",
      "Total Loss: 4280.755966067314\n",
      "Total Loss: 4271.30956864357\n",
      "Total Loss: 4289.656920075417\n",
      "Total Loss: 4317.583520054817\n",
      "Total Loss: 4302.278427839279\n",
      "Total Loss: 4281.486856341362\n",
      "Total Loss: 4261.885267019272\n",
      "Total Loss: 4284.905401587486\n",
      "Total Loss: 4265.044691443443\n",
      "Total Loss: 4342.223200559616\n",
      "Total Loss: 4308.359956860542\n",
      "Total Loss: 4320.4041529893875\n",
      "Total Loss: 4289.565195083618\n",
      "Total Loss: 4329.416192293167\n",
      "Total Loss: 4296.28694319725\n",
      "Total Loss: 4255.612934350967\n",
      "Total Loss: 4370.094703555107\n",
      "Total Loss: 4325.161243796349\n",
      "Total Loss: 4310.935756921768\n",
      "Total Loss: 4300.77319920063\n",
      "Total Loss: 4275.722994089127\n",
      "Total Loss: 4327.440641403198\n",
      "Total Loss: 4302.605435013771\n",
      "Total Loss: 4297.333374142647\n",
      "Total Loss: 4367.20676279068\n",
      "Total Loss: 4259.301837801933\n",
      "Total Loss: 4278.469619750977\n",
      "Total Loss: 4267.016275525093\n",
      "Total Loss: 4255.329446196556\n",
      "Total Loss: 4289.759543180466\n",
      "Total Loss: 4313.255322575569\n",
      "Total Loss: 4298.615732073784\n",
      "Total Loss: 4306.833470702171\n",
      "Total Loss: 4304.966925024986\n",
      "Total Loss: 4281.863197207451\n",
      "Total Loss: 4304.52862238884\n",
      "Total Loss: 4234.222230792046\n",
      "Total Loss: 4308.366752147675\n",
      "Total Loss: 4283.026298046112\n",
      "Total Loss: 4312.275294303894\n",
      "Total Loss: 4331.9511152505875\n",
      "Total Loss: 4329.242048382759\n",
      "Total Loss: 4289.171163082123\n",
      "Epoch 2\n",
      "Total Loss: 4007.5422043800354\n",
      "Total Loss: 4277.359098315239\n",
      "Total Loss: 4260.789814591408\n",
      "Total Loss: 4299.4112939834595\n",
      "Total Loss: 4263.3196305036545\n",
      "Total Loss: 4291.846559524536\n",
      "Total Loss: 4248.836351633072\n",
      "Total Loss: 4252.469676375389\n",
      "Total Loss: 4279.356631278992\n",
      "Total Loss: 4246.286582827568\n",
      "Total Loss: 4228.501508593559\n",
      "Total Loss: 4342.009778141975\n",
      "Total Loss: 4259.1374604702\n",
      "Total Loss: 4247.296293258667\n",
      "Total Loss: 4247.1401232481\n",
      "Total Loss: 4291.648640036583\n",
      "Total Loss: 4265.477934360504\n",
      "Total Loss: 4307.717603445053\n",
      "Total Loss: 4230.4218336343765\n",
      "Total Loss: 4328.229480981827\n",
      "Total Loss: 4305.988148450851\n",
      "Total Loss: 4271.535228252411\n",
      "Total Loss: 4270.140144228935\n",
      "Total Loss: 4313.964668035507\n",
      "Total Loss: 4273.253069281578\n",
      "Total Loss: 4271.778673529625\n",
      "Total Loss: 4302.57212972641\n",
      "Total Loss: 4249.118148684502\n",
      "Total Loss: 4265.946432352066\n",
      "Total Loss: 4228.55008995533\n",
      "Total Loss: 4249.59604549408\n",
      "Total Loss: 4246.388969421387\n",
      "Total Loss: 4302.304688572884\n",
      "Total Loss: 4247.325929403305\n",
      "Total Loss: 4246.65047109127\n",
      "Total Loss: 4240.187620639801\n",
      "Total Loss: 4261.436554551125\n",
      "Total Loss: 4261.897515177727\n",
      "Total Loss: 4318.315379023552\n",
      "Total Loss: 4240.499383449554\n",
      "Total Loss: 4255.652099967003\n",
      "Total Loss: 4280.5795286893845\n",
      "Total Loss: 4282.56286752224\n",
      "Total Loss: 4251.502552747726\n",
      "Total Loss: 4258.032348513603\n",
      "Total Loss: 4287.751399755478\n",
      "Total Loss: 4253.824382781982\n",
      "Total Loss: 4272.524609208107\n",
      "Total Loss: 4270.120996713638\n",
      "Total Loss: 4234.437302350998\n",
      "Total Loss: 4277.634377002716\n",
      "Total Loss: 4270.108242630959\n",
      "Total Loss: 4278.018029212952\n",
      "Total Loss: 4248.664000749588\n",
      "Total Loss: 4199.823982477188\n",
      "Total Loss: 4239.2700835466385\n",
      "Total Loss: 4258.000896334648\n",
      "Total Loss: 4223.535116791725\n",
      "Total Loss: 4320.824913740158\n",
      "Total Loss: 4286.316455125809\n",
      "Total Loss: 4294.5185334682465\n",
      "Total Loss: 4267.218071758747\n",
      "Total Loss: 4312.5547688007355\n",
      "Total Loss: 4290.283388853073\n",
      "Total Loss: 4240.68130671978\n",
      "Total Loss: 4238.077905297279\n",
      "Total Loss: 4275.107984185219\n",
      "Total Loss: 4265.445486187935\n",
      "Total Loss: 4275.601676464081\n",
      "Total Loss: 4254.759644031525\n",
      "Total Loss: 4274.177675843239\n",
      "Total Loss: 4318.679494738579\n",
      "Total Loss: 4276.856855869293\n",
      "Total Loss: 4315.389488697052\n",
      "Total Loss: 4269.616728901863\n",
      "Total Loss: 4201.268022894859\n",
      "Total Loss: 4294.282645583153\n",
      "Total Loss: 4232.390094876289\n",
      "Epoch 3\n",
      "Total Loss: 3912.7123223543167\n",
      "Total Loss: 4234.603217244148\n",
      "Total Loss: 4176.950001239777\n",
      "Total Loss: 4207.086956143379\n",
      "Total Loss: 4283.589428663254\n",
      "Total Loss: 4234.627148747444\n",
      "Total Loss: 4269.143067955971\n",
      "Total Loss: 4299.532429933548\n",
      "Total Loss: 4261.796504735947\n",
      "Total Loss: 4287.348826646805\n",
      "Total Loss: 4253.413242220879\n",
      "Total Loss: 4251.505583047867\n",
      "Total Loss: 4249.656734824181\n",
      "Total Loss: 4254.658475279808\n",
      "Total Loss: 4214.3684549331665\n",
      "Total Loss: 4300.289891004562\n",
      "Total Loss: 4255.8621554374695\n",
      "Total Loss: 4269.679382324219\n",
      "Total Loss: 4286.06168448925\n",
      "Total Loss: 4201.134723067284\n",
      "Total Loss: 4260.240335345268\n",
      "Total Loss: 4258.752303481102\n",
      "Total Loss: 4258.050079464912\n",
      "Total Loss: 4260.00655400753\n",
      "Total Loss: 4254.095475792885\n",
      "Total Loss: 4300.493585109711\n",
      "Total Loss: 4290.175086736679\n",
      "Total Loss: 4294.176082611084\n",
      "Total Loss: 4274.60052973032\n",
      "Total Loss: 4245.796814203262\n",
      "Total Loss: 4253.278417944908\n",
      "Total Loss: 4265.327705740929\n",
      "Total Loss: 4298.704735636711\n",
      "Total Loss: 4301.915547132492\n",
      "Total Loss: 4230.375412583351\n",
      "Total Loss: 4282.901872634888\n",
      "Total Loss: 4243.35526740551\n",
      "Total Loss: 4222.80276954174\n",
      "Total Loss: 4275.9606338739395\n",
      "Total Loss: 4302.226258993149\n",
      "Total Loss: 4272.706393837929\n",
      "Total Loss: 4276.3468323946\n",
      "Total Loss: 4258.832402706146\n",
      "Total Loss: 4305.351993441582\n",
      "Total Loss: 4261.2399253845215\n",
      "Total Loss: 4248.048988342285\n",
      "Total Loss: 4287.473772764206\n",
      "Total Loss: 4279.946156740189\n",
      "Total Loss: 4296.7760199308395\n",
      "Total Loss: 4299.979399859905\n",
      "Total Loss: 4277.986487030983\n",
      "Total Loss: 4306.332928538322\n",
      "Total Loss: 4280.691186785698\n",
      "Total Loss: 4298.705341339111\n",
      "Total Loss: 4279.068532466888\n",
      "Total Loss: 4240.521919965744\n",
      "Total Loss: 4265.262030482292\n",
      "Total Loss: 4260.339281797409\n",
      "Total Loss: 4279.8718411922455\n",
      "Total Loss: 4262.955808520317\n",
      "Total Loss: 4333.232449531555\n",
      "Total Loss: 4273.935764789581\n",
      "Total Loss: 4288.914673924446\n",
      "Total Loss: 4275.781764268875\n",
      "Total Loss: 4254.1882791519165\n",
      "Total Loss: 4268.750709652901\n",
      "Total Loss: 4293.398198366165\n",
      "Total Loss: 4270.225448131561\n",
      "Total Loss: 4303.361843585968\n",
      "Total Loss: 4251.495229482651\n",
      "Total Loss: 4296.7743862867355\n",
      "Total Loss: 4263.820643186569\n",
      "Total Loss: 4257.781823396683\n",
      "Total Loss: 4304.204125165939\n",
      "Total Loss: 4299.597559571266\n",
      "Total Loss: 4272.863943099976\n",
      "Total Loss: 4280.464654326439\n",
      "Total Loss: 4239.655325293541\n"
     ]
    }
   ],
   "source": [
    "model = train(EPOCHS, LOADER, BATCH_SIZE, TOKENIZER, MODEL, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model...\n"
     ]
    }
   ],
   "source": [
    "save_model(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_from_top_k_top_n(probs, k=50, p=0.8):\n",
    "    ind = np.argpartition(probs, -k)[-k:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = {i: top_prob[idx] for idx,i in enumerate(ind)}\n",
    "    sorted_top_prob = {k: v for k, v in sorted(top_prob.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    t=0\n",
    "    f=[]\n",
    "    pr = []\n",
    "    for k,v in sorted_top_prob.items():\n",
    "        t+=v\n",
    "        f.append(k)\n",
    "        pr.append(v)\n",
    "        if t>=p:\n",
    "            break\n",
    "    top_prob = pr / np.sum(pr)\n",
    "    token_id = np.random.choice(f, 1, p = top_prob)\n",
    "\n",
    "    return int(token_id)\n",
    "\n",
    "def load_models(model_name):\n",
    "    print ('Loading Trained GPT-2 Model')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "    model_path = model_name\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(tokenizer, model, sentences, label):\n",
    "    with torch.no_grad():\n",
    "        for idx in range(sentences):\n",
    "            finished = False\n",
    "            cur_ids = torch.tensor(tokenizer.encode(label)).unsqueeze(0).to(DEVICE)\n",
    "            for i in range(128):\n",
    "                outputs = model(cur_ids, labels=cur_ids)\n",
    "                loss, logits = outputs[:2]\n",
    "\n",
    "                softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
    "\n",
    "                if i < 5:\n",
    "                    n = 10\n",
    "                else:\n",
    "                    n = 5\n",
    "\n",
    "                next_token_id = choose_from_top_k_top_n(softmax_logits.to('cpu').numpy()) #top-k-top-n sampling\n",
    "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(DEVICE) * next_token_id], dim = 1)\n",
    "\n",
    "                if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
    "                    finished = True\n",
    "                    break\n",
    "\n",
    "            if finished:         \n",
    "                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "                output_text = tokenizer.decode(output_list)\n",
    "                print (output_text)\n",
    "            else:\n",
    "                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "                output_text = tokenizer.decode(output_list)\n",
    "                print (output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal: your comments on trudeau are irrelevant. <|endoftext|>\n",
      "normal: a good friend and friend of mine has a good job. <|endoftext|>\n",
      "normal: he would take the initiative to give his party a good old fashioned look in the light of a progressive agenda. <|endoftext|>\n",
      "normal: we had the war and it was better than the slavery. <|endoftext|>\n",
      "normal: he will continue to pay the tax they just made. <|endoftext|>\n",
      "normal: maybe in the future i am, but for now i want to learn, and work on, myself. <|endoftext|>\n",
      "normal: its not a coincidence that many countries do not want a muslim invasion. <|endoftext|>\n",
      "normal: to show how wrong i am with my own writing here. <|endoftext|>\n",
      "normal: thats why the trudeau is a trudeau. <|endoftext|>\n",
      "normal: the majority of the time she can be fairly described as a nazi sympathizer. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "generate(TOKENIZER, model, 10, 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: he is just another corrupt politician. <|endoftext|>\n",
      "toxic: that was a joke, and this is all, he is a fool. <|endoftext|>\n",
      "toxic: im not even sure how you feel about that. <|endoftext|>\n",
      "toxic: the americans need to be sent back to a land of no freedom of speech. <|endoftext|>\n",
      "toxic: you are stupid. <|endoftext|>\n",
      "toxic: when it comes to stupidity, that last bit matters most. <|endoftext|>\n",
      "toxic: your stupidity and lack of common sense is the real problem with america. <|endoftext|>\n",
      "toxic: i dont like this post, however, it seems alluding to the fact that, yes, i do love black people, it just seems so stupid. <|endoftext|>\n",
      "toxic: you think theyll shoot you. <|endoftext|>\n",
      "toxic: you are a big fucking fucker who has a bad attitude. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "generate(TOKENIZER, model, 10, 'toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal: and if theres no evidence to support it, then it is silly to argue. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "generate(TOKENIZER, model, 1, 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
