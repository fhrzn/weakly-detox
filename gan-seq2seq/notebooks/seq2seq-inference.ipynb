{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/jovyan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jovyan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jovyan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jovyan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jovyan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jovyan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate config dictionary...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from dataloader import load_dataset\n",
    "from gan_helper import get_cycle_gan_network, get_criterions, get_optimizers, get_schedulers\n",
    "from config import *\n",
    "from utils import load_model\n",
    "from train_helper import evaluate_cycle_gan\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from utils import get_sentence_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:4' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, iterators = load_dataset(BATCH_SIZE, DEVICE, mode='inference')\n",
    "train_iterator, dev_iterator, test_iterator = iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_INPUT_DIM = len(source.vocab)\n",
    "G_OUTPUT_DIM = len(source.vocab)\n",
    "SOS_IDX = source.vocab.stoi['<sos>']\n",
    "PAD_IDX = source.vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_g_ab, criterion_g_ba, criterion_gan, \\\n",
    "    criterion_discriminator, criterion_cycle, \\\n",
    "    criterion_identity = get_criterions(PAD_IDX, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained generator...\n",
      "load pretrained discriminator...\n"
     ]
    }
   ],
   "source": [
    "g_ab, g_ba, d_a, d_b = get_cycle_gan_network(G_INPUT_DIM, \n",
    "                                             G_OUTPUT_DIM, \n",
    "                                             DEVICE, \n",
    "                                             PAD_IDX, \n",
    "                                             SOS_IDX,\n",
    "                                             True, \n",
    "                                             True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "classifier_ckpt = 's-nlp/roberta_toxicity_classifier' # 'roberta-base'\n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(classifier_ckpt)\n",
    "tokenizer = AutoTokenizer.from_pretrained(classifier_ckpt)\n",
    "# load_model(model=classifier, path='../classifier/model/ft-robertoxic-classifier.pth')\n",
    "classifier = classifier.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f19e748a514e7881ced09a95f9ed29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss_gan_ab: 0.6930230182560184 | loss_gan_ba: 0.7931372904324834 | bleu_score_a: 92.17068387589649 | bleu_score_b: 95.20920725869465\n"
     ]
    }
   ],
   "source": [
    "loss_gan_ab, loss_gan_ba, bleu_score_a, bleu_score_b, sta_ab, sta_ba = evaluate_cycle_gan(\n",
    "    source,\n",
    "    DEVICE,\n",
    "    g_ab,\n",
    "    g_ba,\n",
    "    d_a,\n",
    "    d_b,\n",
    "    iterators[2], # Test data\n",
    "    criterion_gan,\n",
    "    sta_tokenizer=tokenizer,\n",
    "    sta_model=classifier\n",
    ")\n",
    "print(f'\\nloss_gan_ab: {loss_gan_ab} | loss_gan_ba: {loss_gan_ba} | bleu_score_a: {bleu_score_a} | bleu_score_b: {bleu_score_b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text):\n",
    "    tokenized = tokenizer(text,\n",
    "                         truncation=True,\n",
    "                         max_length=128,\n",
    "                         padding='max_length',\n",
    "                         return_tensors='pt')\n",
    "    tokenized = {k: v.to(DEVICE) for k, v in tokenized.items()}\n",
    "    with torch.no_grad():\n",
    "        out = classifier(**tokenized).logits\n",
    "    proba = torch.softmax(out, dim=1).squeeze()\n",
    "    label = torch.argmax(proba)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    del tokenized\n",
    "    \n",
    "    return {'normal_proba': proba[0].item(),\n",
    "            'toxic_proba': proba[1].item(),\n",
    "            'predicted_label': label.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3a1cd0fecd4d17b706fadbf004f14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_ab = []\n",
    "res_ba = []\n",
    "\n",
    "d_a.eval()\n",
    "d_b.eval()\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_iterator)) as pbar:\n",
    "        for i, batch in enumerate(test_iterator):\n",
    "            pbar.update(1)\n",
    "            \n",
    "            real_a = batch.src.to(DEVICE)\n",
    "            real_b = batch.trg.to(DEVICE)\n",
    "            \n",
    "            _, fake_b = g_ab(real_a, 0)\n",
    "            _, fake_a = g_ba(real_b, 0)\n",
    "            \n",
    "            # Save A to B scores     \n",
    "            real_a_sentences = get_sentence_from_tensor(source, real_a)\n",
    "            fake_b_sentences = get_sentence_from_tensor(source, fake_b)\n",
    "            for real_a_sentence, fake_b_sentence in zip(real_a_sentences, fake_b_sentences):\n",
    "                bleu_score = sentence_bleu([real_a_sentence], fake_b_sentence, smoothing_function=smoother.method4) * 100\n",
    "                sta_score = predict_label(' '.join(fake_b_sentence))\n",
    "                res_ab.append({\"source\": \" \".join(real_a_sentence), \n",
    "                               \"gen\": \" \".join(fake_b_sentence), \n",
    "                               \"Bleu Score\": bleu_score,\n",
    "                               \"Normal Probability\": sta_score['normal_proba'],\n",
    "                               \"Toxic Probability\": sta_score['toxic_proba'],\n",
    "                               \"Predicted Label\": sta_score['predicted_label']})\n",
    "\n",
    "            # Save B to A scores   \n",
    "            real_b_sentences = get_sentence_from_tensor(source, real_b)\n",
    "            fake_a_sentences = get_sentence_from_tensor(source, fake_a)\n",
    "            for real_b_sentence, fake_a_sentence in zip(real_b_sentences, fake_a_sentences):\n",
    "                bleu_score = sentence_bleu([real_b_sentence], fake_a_sentence, smoothing_function=smoother.method4) * 100\n",
    "                sta_score = predict_label(' '.join(fake_a_sentence))\n",
    "                res_ba.append({\"source\": \" \".join(real_b_sentence), \n",
    "                               \"gen\": \" \".join(fake_a_sentence), \n",
    "                               \"Bleu Score\": bleu_score,\n",
    "                               \"Normal Probability\": sta_score['normal_proba'],\n",
    "                               \"Toxic Probability\": sta_score['toxic_proba'],\n",
    "                               \"Predicted Label\": sta_score['predicted_label']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>gen</th>\n",
       "      <th>Bleu Score</th>\n",
       "      <th>Normal Probability</th>\n",
       "      <th>Toxic Probability</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>Original Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stop censoring me you douche &lt;unk&gt; .</td>\n",
       "      <td>stop censoring me you renounced &lt;unk&gt; .</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.003381</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i think wikipedia is a crap .</td>\n",
       "      <td>i think wikipedia is a crap .</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.010503</td>\n",
       "      <td>0.989497</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>so , haha u losers lol !</td>\n",
       "      <td>so , haha u losers lol !</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>0.990633</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wake up your ignorance is dangerous .</td>\n",
       "      <td>wake up your ignorance is dangerous .</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.006464</td>\n",
       "      <td>0.993536</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump just employs racists and bigots .</td>\n",
       "      <td>trump just employs racists and bigots .</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>0.994389</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>: : mooch ass grassy ass .</td>\n",
       "      <td>: : mooch ass grassy ass .</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.995338</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>what s up prissy nigga , .</td>\n",
       "      <td>what s up yahoo potheads , .</td>\n",
       "      <td>33.652250</td>\n",
       "      <td>0.978782</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>throw him down the nasty well .</td>\n",
       "      <td>throw him down the nasty well .</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.996056</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what utter hypocrisy and bs ! .</td>\n",
       "      <td>what utter hypocrisy and bs ! .</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>0.996142</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>so you know his penis length ?</td>\n",
       "      <td>so you know his penis length ?</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.996769</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>turn the damned &lt;unk&gt; back on .</td>\n",
       "      <td>turn the damned &lt;unk&gt; back on .</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.995063</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>you go look at moms ass &lt;unk&gt;</td>\n",
       "      <td>you go look at moms ass &lt;unk&gt;</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.998219</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>are all your heroes cowardly psychos ?</td>\n",
       "      <td>are all your heroes cowardly crooks ?</td>\n",
       "      <td>64.345888</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.995445</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;unk&gt; your head between your legs .</td>\n",
       "      <td>&lt;unk&gt; your head between your legs .</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.997798</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>he sucks cuz he blocked me .</td>\n",
       "      <td>he sucks cuz he blocked me .</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>0.997159</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>do nt be silly , hell .</td>\n",
       "      <td>do nt be silly , hell .</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.997699</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>read that fckin article you fggot !</td>\n",
       "      <td>read that postnational article you areyears !</td>\n",
       "      <td>31.100460</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>and lying dickheads came from serbia !</td>\n",
       "      <td>and lying mosaic came from va !</td>\n",
       "      <td>31.100460</td>\n",
       "      <td>0.992867</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>and he s as dirty as comey</td>\n",
       "      <td>and he s as dirty as comey</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.997410</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>of the biggest bitches on the universe</td>\n",
       "      <td>of the biggest implant on the universe</td>\n",
       "      <td>45.009231</td>\n",
       "      <td>0.999818</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source  \\\n",
       "0      stop censoring me you douche <unk> .   \n",
       "1             i think wikipedia is a crap .   \n",
       "2                  so , haha u losers lol !   \n",
       "3     wake up your ignorance is dangerous .   \n",
       "4   trump just employs racists and bigots .   \n",
       "5                : : mooch ass grassy ass .   \n",
       "6                what s up prissy nigga , .   \n",
       "7           throw him down the nasty well .   \n",
       "8           what utter hypocrisy and bs ! .   \n",
       "9            so you know his penis length ?   \n",
       "10          turn the damned <unk> back on .   \n",
       "11            you go look at moms ass <unk>   \n",
       "12   are all your heroes cowardly psychos ?   \n",
       "13      <unk> your head between your legs .   \n",
       "14             he sucks cuz he blocked me .   \n",
       "15                  do nt be silly , hell .   \n",
       "16      read that fckin article you fggot !   \n",
       "17   and lying dickheads came from serbia !   \n",
       "18               and he s as dirty as comey   \n",
       "19   of the biggest bitches on the universe   \n",
       "\n",
       "                                              gen  Bleu Score  \\\n",
       "0         stop censoring me you renounced <unk> .   48.892302   \n",
       "1                   i think wikipedia is a crap .  100.000000   \n",
       "2                        so , haha u losers lol !  100.000000   \n",
       "3           wake up your ignorance is dangerous .  100.000000   \n",
       "4         trump just employs racists and bigots .  100.000000   \n",
       "5                      : : mooch ass grassy ass .  100.000000   \n",
       "6                    what s up yahoo potheads , .   33.652250   \n",
       "7                 throw him down the nasty well .  100.000000   \n",
       "8                 what utter hypocrisy and bs ! .  100.000000   \n",
       "9                  so you know his penis length ?  100.000000   \n",
       "10                turn the damned <unk> back on .  100.000000   \n",
       "11                  you go look at moms ass <unk>  100.000000   \n",
       "12          are all your heroes cowardly crooks ?   64.345888   \n",
       "13            <unk> your head between your legs .  100.000000   \n",
       "14                   he sucks cuz he blocked me .  100.000000   \n",
       "15                        do nt be silly , hell .  100.000000   \n",
       "16  read that postnational article you areyears !   31.100460   \n",
       "17                and lying mosaic came from va !   31.100460   \n",
       "18                     and he s as dirty as comey  100.000000   \n",
       "19         of the biggest implant on the universe   45.009231   \n",
       "\n",
       "    Normal Probability  Toxic Probability  Predicted Label  Original Length  \n",
       "0             0.996619           0.003381                0                7  \n",
       "1             0.010503           0.989497                1                7  \n",
       "2             0.009367           0.990633                1                7  \n",
       "3             0.006464           0.993536                1                7  \n",
       "4             0.005610           0.994389                1                7  \n",
       "5             0.004662           0.995338                1                7  \n",
       "6             0.978782           0.021218                0                7  \n",
       "7             0.003944           0.996056                1                7  \n",
       "8             0.003858           0.996142                1                7  \n",
       "9             0.003231           0.996769                1                7  \n",
       "10            0.004937           0.995063                1                7  \n",
       "11            0.001781           0.998219                1                7  \n",
       "12            0.004555           0.995445                1                7  \n",
       "13            0.002202           0.997798                1                7  \n",
       "14            0.002841           0.997159                1                7  \n",
       "15            0.002301           0.997699                1                7  \n",
       "16            0.999950           0.000050                0                7  \n",
       "17            0.992867           0.007133                0                7  \n",
       "18            0.002590           0.997410                1                7  \n",
       "19            0.999818           0.000182                0                7  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ab = pd.DataFrame(res_ab)\n",
    "df_ab[\"Original Length\"] = df_ab[\"source\"].apply(lambda x: len(x.split()))\n",
    "# df_ab_bleu_by_length = pd.DataFrame(df_ab.groupby(['Original Length'])['Bleu Score'].mean())\n",
    "# df_ab_bleu_by_length = df_ab_bleu_by_length.reset_index()\n",
    "# df_ab_bleu_by_length[\"network_type\"] = \"G_AB\"\n",
    "df_ab.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>gen</th>\n",
       "      <th>Bleu Score</th>\n",
       "      <th>Normal Probability</th>\n",
       "      <th>Toxic Probability</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>Original Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>who cares what this dirtbag thinks !</td>\n",
       "      <td>who cares what this taxs thinks !</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.997833</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>hold your goddamn horses , already .</td>\n",
       "      <td>hold your avatar horses , already .</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.998929</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3966</th>\n",
       "      <td>be an asshole about it though )</td>\n",
       "      <td>be an alleging about it though )</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.999899</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>do nt fck with my friends .</td>\n",
       "      <td>do nt laughed with my friends .</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>sovereign state my fat hairy nuts .</td>\n",
       "      <td>sovereign state my fat hydroelectric nuts .</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.987328</td>\n",
       "      <td>0.012672</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4305</th>\n",
       "      <td>anyone other than a smelly liberal .</td>\n",
       "      <td>anyone other than a peice liberal .</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.998125</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>what da fucks a gigi ? !</td>\n",
       "      <td>what da venezuelas a gigi ? !</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.997184</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>what a twat that fella is .</td>\n",
       "      <td>what a faceless that fella is .</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.995012</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>will wright is a fuckin genius !</td>\n",
       "      <td>will wright is a wahine genius !</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.992874</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>her moms boobs are better though .</td>\n",
       "      <td>her moms foes are better though .</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.996664</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>you son of a bitch ! digit</td>\n",
       "      <td>you son of a charming ! digit</td>\n",
       "      <td>48.892302</td>\n",
       "      <td>0.964330</td>\n",
       "      <td>0.035670</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>he is a right git with a smelly cunt if ya ask...</td>\n",
       "      <td>he is a right tarred with a gi wweek if ya ask...</td>\n",
       "      <td>48.415247</td>\n",
       "      <td>0.994125</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>by the way i fucked ur mum in the arse hole</td>\n",
       "      <td>by the way i googled ur mum in the wealthiest ...</td>\n",
       "      <td>48.326978</td>\n",
       "      <td>0.981873</td>\n",
       "      <td>0.018127</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>you re an asshole how s that for a stereotype .</td>\n",
       "      <td>you re an assertive how s that for a variation .</td>\n",
       "      <td>48.326978</td>\n",
       "      <td>0.999577</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4880</th>\n",
       "      <td>whereas trumpski is an all the time pussy grab...</td>\n",
       "      <td>whereas ferguson is an all the time chokes gra...</td>\n",
       "      <td>46.713798</td>\n",
       "      <td>0.911955</td>\n",
       "      <td>0.088045</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>hahaha what a looser you are !</td>\n",
       "      <td>hahaha what a stint you are !</td>\n",
       "      <td>45.009231</td>\n",
       "      <td>0.998367</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>of the biggest bitches on the universe</td>\n",
       "      <td>of the biggest implant on the universe</td>\n",
       "      <td>45.009231</td>\n",
       "      <td>0.999818</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>p. s. : fuck your sock puppets , you unscrupul...</td>\n",
       "      <td>p. s. : feigned your sock puppets , you redeem...</td>\n",
       "      <td>43.668354</td>\n",
       "      <td>0.990411</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>turn it up you fuckin foo .</td>\n",
       "      <td>turn it up you peddle diverdave .</td>\n",
       "      <td>43.472087</td>\n",
       "      <td>0.989500</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>eat a bag of dicks faggot .</td>\n",
       "      <td>eat a bag of caricatures tweaked .</td>\n",
       "      <td>43.472087</td>\n",
       "      <td>0.999554</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 source  \\\n",
       "213                who cares what this dirtbag thinks !   \n",
       "641                hold your goddamn horses , already .   \n",
       "3966                    be an asshole about it though )   \n",
       "978                         do nt fck with my friends .   \n",
       "507                 sovereign state my fat hairy nuts .   \n",
       "4305               anyone other than a smelly liberal .   \n",
       "4009                           what da fucks a gigi ? !   \n",
       "3599                        what a twat that fella is .   \n",
       "3595                   will wright is a fuckin genius !   \n",
       "3593                 her moms boobs are better though .   \n",
       "714                          you son of a bitch ! digit   \n",
       "5754  he is a right git with a smelly cunt if ya ask...   \n",
       "4868        by the way i fucked ur mum in the arse hole   \n",
       "2129    you re an asshole how s that for a stereotype .   \n",
       "4880  whereas trumpski is an all the time pussy grab...   \n",
       "649                      hahaha what a looser you are !   \n",
       "19               of the biggest bitches on the universe   \n",
       "6038  p. s. : fuck your sock puppets , you unscrupul...   \n",
       "614                         turn it up you fuckin foo .   \n",
       "665                         eat a bag of dicks faggot .   \n",
       "\n",
       "                                                    gen  Bleu Score  \\\n",
       "213                   who cares what this taxs thinks !   48.892302   \n",
       "641                 hold your avatar horses , already .   48.892302   \n",
       "3966                   be an alleging about it though )   48.892302   \n",
       "978                     do nt laughed with my friends .   48.892302   \n",
       "507         sovereign state my fat hydroelectric nuts .   48.892302   \n",
       "4305                anyone other than a peice liberal .   48.892302   \n",
       "4009                      what da venezuelas a gigi ? !   48.892302   \n",
       "3599                    what a faceless that fella is .   48.892302   \n",
       "3595                   will wright is a wahine genius !   48.892302   \n",
       "3593                  her moms foes are better though .   48.892302   \n",
       "714                       you son of a charming ! digit   48.892302   \n",
       "5754  he is a right tarred with a gi wweek if ya ask...   48.415247   \n",
       "4868  by the way i googled ur mum in the wealthiest ...   48.326978   \n",
       "2129   you re an assertive how s that for a variation .   48.326978   \n",
       "4880  whereas ferguson is an all the time chokes gra...   46.713798   \n",
       "649                       hahaha what a stint you are !   45.009231   \n",
       "19               of the biggest implant on the universe   45.009231   \n",
       "6038  p. s. : feigned your sock puppets , you redeem...   43.668354   \n",
       "614                   turn it up you peddle diverdave .   43.472087   \n",
       "665                  eat a bag of caricatures tweaked .   43.472087   \n",
       "\n",
       "      Normal Probability  Toxic Probability  Predicted Label  Original Length  \n",
       "213             0.997833           0.002167                0                7  \n",
       "641             0.998929           0.001071                0                7  \n",
       "3966            0.999899           0.000101                0                7  \n",
       "978             0.999947           0.000053                0                7  \n",
       "507             0.987328           0.012672                0                7  \n",
       "4305            0.998125           0.001875                0                7  \n",
       "4009            0.997184           0.002816                0                7  \n",
       "3599            0.995012           0.004988                0                7  \n",
       "3595            0.992874           0.007126                0                7  \n",
       "3593            0.996664           0.003336                0                7  \n",
       "714             0.964330           0.035670                0                7  \n",
       "5754            0.994125           0.005875                0               14  \n",
       "4868            0.981873           0.018127                0               11  \n",
       "2129            0.999577           0.000423                0               11  \n",
       "4880            0.911955           0.088045                0               10  \n",
       "649             0.998367           0.001633                0                7  \n",
       "19              0.999818           0.000182                0                7  \n",
       "6038            0.990411           0.009589                0               12  \n",
       "614             0.989500           0.010500                0                7  \n",
       "665             0.999554           0.000446                0                7  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ab[(df_ab['Bleu Score'] < 50) & \\\n",
    "      (df_ab['Predicted Label'] == 0) & \\\n",
    "      (~df_ab[\"source\"].str.contains('<unk>')) & \\\n",
    "      (~df_ab[\"gen\"].str.contains('<unk>'))].sort_values(by=['Bleu Score'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STA: 0.1089\n",
      "BLEU: 92.19621573635399\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(f\"STA: {accuracy_score(np.zeros(len(df_ab), dtype=int), df_ab['Predicted Label'].to_numpy())}\")\n",
    "print(f\"BLEU: {df_ab['Bleu Score'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ab[['source', 'gen']].to_csv('../data/seq2seq/output/detoxified-2.txt', sep='\\t', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
